{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set path because otherwise it does not find price_simulator packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "filepath_pc = r\"C:\\Users\\Thomas Gausmann\\sciebo - Gausmann, Thomas (t_gaus04@uni-muenster.de)@uni-muenster.sciebo.de\\Masterarbeit\\price_simulator\"\n",
    "filepath_laptop = r\"C:\\Users\\gausm\\sciebo - Gausmann, Thomas (t_gaus04@uni-muenster.de)@uni-muenster.sciebo.de\\Masterarbeit\\price_simulator\"\n",
    "os.chdir(filepath_laptop)\n",
    "sys.path.append(filepath_laptop)\n",
    "import numpy as np\n",
    "import attr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "packages packages (homemade) packages\n",
    "\n",
    "Note: do not run tf and pyTorch together, calamity ensues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Tuple\n",
    "\n",
    "import attr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from price_simulator.src.algorithm.agents.simple import AgentStrategy\n",
    "from price_simulator.src.algorithm.policies import EpsilonGreedy, ExplorationStrategy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc1(out[:, -1, :])\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create the LSTM agent and its procedures\n",
    "\n",
    "Kept close to DQNAgent in approximate.py, less target network and replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@attr.s\n",
    "class SimpleLSTMAgent(AgentStrategy):\n",
    "    \"\"\"Simplified LSTM Agent using sequences of past states\"\"\"\n",
    "\n",
    "    # LSTM Network\n",
    "    lstm: LSTMModel = attr.ib(default=None)\n",
    "    hidden_nodes: int = attr.ib(default=32)\n",
    "    sequence_length: int = attr.ib(default=5)  # Number of past states to use\n",
    "\n",
    "    # General\n",
    "    decision: ExplorationStrategy = attr.ib(factory=EpsilonGreedy)\n",
    "    discount: float = attr.ib(default=0.95)\n",
    "    learning_rate: float = attr.ib(default=0.1)\n",
    "\n",
    "    @discount.validator\n",
    "    def check_discount(self, attribute, value):\n",
    "        if not 0 <= value <= 1:\n",
    "            raise ValueError(\"Discount factor must lie in [0,1]\")\n",
    "\n",
    "    @learning_rate.validator\n",
    "    def check_learning_rate(self, attribute, value):\n",
    "        if not 0 <= value < 1:\n",
    "            raise ValueError(\"Learning rate must lie in [0,1)\")\n",
    "\n",
    "    def who_am_i(self) -> str:\n",
    "        return type(self).__name__ + \" (gamma: {}, alpha: {}, policy: {}, quality: {}, mc: {})\".format(\n",
    "            self.discount, self.learning_rate, self.decision.who_am_i(), self.quality, self.marginal_cost\n",
    "        )\n",
    "\n",
    "    def play_price(self, states: List[Tuple[float]], action_space: List[float], n_period: int, t: int) -> float:\n",
    "        \"\"\"Returns an action by either following greedy policy or experimentation.\"\"\"\n",
    "\n",
    "        # init LSTM network if necessary\n",
    "        if not self.lstm:\n",
    "            self.lstm = self.initialize_network(len(states[0]), len(action_space))\n",
    "\n",
    "        # play action\n",
    "        if self.decision.explore(n_period, t):\n",
    "            # if exploration:\n",
    "            # select random action from the action space\n",
    "            return random.choice(action_space) \n",
    "        else:\n",
    "            # otherwise exploit\n",
    "            # scale input sequence between [0,1]. cast to floating point tnesor, add new dimension at position 0, indicating bacth size\n",
    "            states_input = torch.tensor(self.scale_sequence(states, action_space)).float().unsqueeze(0)\n",
    "            # predict action values (Q-Values): detach to ensure no gradients are tracked (i.e. computations below do not affect original tensor),\n",
    "            # returns numpy array\n",
    "            # action values of form [1, num_actions]\n",
    "            action_values = self.lstm(states_input).detach().numpy()\n",
    "            # check for ties\n",
    "            if sum(np.isclose(action_values[0], action_values[0].max())) > 1:\n",
    "                #in case of ties, select randomly\n",
    "                optimal_action_index = np.random.choice(\n",
    "                    np.flatnonzero(np.isclose(action_values[0], action_values[0].max()))\n",
    "                )\n",
    "            else:\n",
    "                # otherwise select maximum action value straight up\n",
    "                optimal_action_index = np.argmax(action_values[0])\n",
    "            return action_space[optimal_action_index]\n",
    "\n",
    "    def learn(\n",
    "        self,\n",
    "        previous_rewards: List[float],\n",
    "        rewards: List[float],\n",
    "        previous_actions: List[float],\n",
    "        actions: List[float],\n",
    "        action_space: List,\n",
    "        previous_states: List[Tuple],\n",
    "        states: List[Tuple],\n",
    "        next_states: List[Tuple],\n",
    "    ):\n",
    "        # Prepare the current state and next state sequences\n",
    "        # scale them [0,1], make them a float and add batch dimension\n",
    "        states_input = torch.tensor(self.scale_sequence(states, action_space)).float().unsqueeze(0)\n",
    "        next_states_input = torch.tensor(self.scale_sequence(next_states, action_space)).float().unsqueeze(0)\n",
    "\n",
    "        # Get max predicted Q values (for next state) from the local model\n",
    "        # item() to select value from tensor\n",
    "        next_optimal_q = self.lstm(next_states_input).max().item()\n",
    "\n",
    "        # Compute Q targets for the current state (Bellman equation)\n",
    "        targets = rewards[-1] + self.discount * next_optimal_q\n",
    "\n",
    "        # Get current Q values from the local model ...\n",
    "        local_estimates = self.lstm(states_input)\n",
    "        # ... and update them with better estimates\n",
    "        action_idx = np.atleast_1d(action_space == actions[-1]).nonzero()[0] # determine index of the action; instead of np.where to avoid deprecated warning\n",
    "        local_estimates[0, action_idx] = targets\n",
    "\n",
    "        # Perform gradient descent step on the local network\n",
    "        optimizer = optim.Adam(self.lstm.parameters(), lr=self.learning_rate)\n",
    "        # due to gradient accumulation, reset if not using batch updates\n",
    "        optimizer.zero_grad()\n",
    "        # compute lossfunction\n",
    "        loss = nn.MSELoss()(local_estimates, local_estimates.clone().detach()) # instead of loss = nn.MSELoss()(local_estimates, torch.tensor(local_estimates).float()) to avoid warning\n",
    "        # backward propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    def initialize_network(self, n_agents: int, n_actions: int):\n",
    "        \"\"\"Create a neural network with one output node per possible action\"\"\"\n",
    "        return LSTMModel(input_size=n_agents, hidden_size=self.hidden_nodes, output_size=n_actions)\n",
    "\n",
    "    def scale_sequence(self, sequences: List[Tuple], action_space: List) -> np.array:\n",
    "        \"\"\"Scale float input sequences to range from 0 to 1.\"\"\"\n",
    "        max_action = max(action_space)\n",
    "        min_action = min(action_space)\n",
    "        return np.array([\n",
    "            np.multiply(np.divide(np.array(seq) - min_action, max_action - min_action), 1) for seq in sequences\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test play price with example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play_price result: 2.0\n"
     ]
    }
   ],
   "source": [
    "def test_play_price():\n",
    "    # Define the possible action space\n",
    "    possible_prices = [1.0, 2.0, 3.0]\n",
    "\n",
    "    # Create an instance of SimpleLSTMAgent with EpsilonGreedy strategy\n",
    "    agent = SimpleLSTMAgent(decision=EpsilonGreedy(eps=0.0))\n",
    "\n",
    "    # Define a sample state sequence with sequence length of 5\n",
    "    state_sequence = [\n",
    "        (1.0, 2.0),\n",
    "        (2.0, 2.0),\n",
    "        (2.0, 2.0),\n",
    "        (1.0, 2.0),\n",
    "        (2.0, 2.0)\n",
    "    ]\n",
    "\n",
    "    # Call the play_price function to get an action\n",
    "    action = agent.play_price(state_sequence, possible_prices, 0, 0)\n",
    "\n",
    "    # Verify that the action is in the possible action space\n",
    "    assert action in possible_prices, f\"Action {action} is not in the possible action space {possible_prices}\"\n",
    "\n",
    "    print(f\"play_price result: {action}\")\n",
    "\n",
    "# Run the test\n",
    "test_play_price()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test wether learning function runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values after learning: [[-0.09762758 -0.12373158 -0.09351548]]\n"
     ]
    }
   ],
   "source": [
    "def test_learn():\n",
    "    # Define the possible action space\n",
    "    possible_prices = [1.0, 2.0, 3.0]\n",
    "\n",
    "    # Create an instance of SimpleLSTMAgent with EpsilonGreedy strategy, no exploration\n",
    "    agent = SimpleLSTMAgent(decision=EpsilonGreedy(eps=0.0))\n",
    "\n",
    "    # Initialize the LSTM network\n",
    "    agent.lstm = agent.initialize_network(2, len(possible_prices))\n",
    "\n",
    "    # Define a sample state sequence\n",
    "    state_sequence = [\n",
    "        (1.0, 2.0),\n",
    "        (2.0, 3.0),\n",
    "        (3.0, 1.0),\n",
    "        (2.0, 1.0),\n",
    "        (1.0, 3.0)\n",
    "    ]\n",
    "    \n",
    "    # Define the next state sequence\n",
    "    next_state_sequence = [\n",
    "        (2.0, 1.0),\n",
    "        (3.0, 2.0),\n",
    "        (1.0, 2.0),\n",
    "        (3.0, 1.0),\n",
    "        (2.0, 3.0)\n",
    "    ]\n",
    "\n",
    "    # Sample actions and rewards\n",
    "    previous_rewards = [1.0]\n",
    "    rewards = [10.0]\n",
    "    previous_actions = [0.0]\n",
    "    actions = [1.0]\n",
    "\n",
    "    # Call the learn function to update Q-values\n",
    "    agent.learn(\n",
    "        previous_rewards=previous_rewards, # not used\n",
    "        rewards=rewards,\n",
    "        previous_actions=previous_actions, # not used\n",
    "        actions=actions,\n",
    "        action_space=possible_prices,\n",
    "        previous_states=state_sequence, # not used\n",
    "        states=state_sequence,\n",
    "        next_states=next_state_sequence\n",
    "    )\n",
    "    \n",
    "    # Check the Q-values\n",
    "    state_input = torch.tensor(agent.scale_sequence(state_sequence, possible_prices)).float().unsqueeze(0)\n",
    "    action_values = agent.lstm(state_input).detach().numpy()\n",
    "\n",
    "    print(f\"Q-values after learning: {action_values}\")\n",
    "\n",
    "# Run the test\n",
    "test_learn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent                                                                                            Average Price    Nash Price    Monopoly Price    Average Profit Gain    Nash Profit    Monopoly Profit\n",
      "---------------------------------------------------------------------------------------------  ---------------  ------------  ----------------  ---------------------  -------------  -----------------\n",
      "AlwaysDefectAgent                                                                                      1.42772       1.47293           1.92498               0.283858       0.222927            0.33749\n",
      "Qlearning (gamma: 0.95, alpha: 0.125, policy: DecreasingEpsilonGreedy, quality: 2.0, mc: 1.0)          1.57339       1.47293           1.92498              -0.285828       0.222927            0.33749\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from price_simulator.src.algorithm import main\n",
    "main.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import price_simulator.src.utils.analyzer as Analyzer\n",
    "from price_simulator.src.algorithm.agents.approximate import DiffDQN\n",
    "from price_simulator.src.algorithm.agents.simple import AlwaysDefectAgent\n",
    "from price_simulator.src.algorithm.agents.tabular import Qlearning\n",
    "from price_simulator.src.algorithm.demand import LogitDemand\n",
    "from price_simulator.src.algorithm.environment import DiscreteSynchronEnvironment\n",
    "from price_simulator.src.algorithm.policies import DecreasingEpsilonGreedy\n",
    "from price_simulator.src.algorithm.agents.lstm_agent import LSTM_Agent  # Import the new LSTM agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent                                                                                            Average Price    Nash Price    Monopoly Price    Average Profit Gain    Nash Profit    Monopoly Profit\n",
      "---------------------------------------------------------------------------------------------  ---------------  ------------  ----------------  ---------------------  -------------  -----------------\n",
      "Qlearning (gamma: 0.95, alpha: 0.125, policy: DecreasingEpsilonGreedy, quality: 2.0, mc: 1.0)          1.67401       1.47293           1.92498               0.41837        0.222927            0.33749\n",
      "Qlearning (gamma: 0.95, alpha: 0.125, policy: DecreasingEpsilonGreedy, quality: 2.0, mc: 1.0)          1.65593       1.47293           1.92498               0.508553       0.222927            0.33749\n"
     ]
    }
   ],
   "source": [
    "def run():\n",
    "    dqn_env = DiscreteSynchronEnvironment(\n",
    "        markup=0.1,\n",
    "        n_periods=100000,\n",
    "        possible_prices=[],\n",
    "        n_prices=15,\n",
    "        demand=LogitDemand(outside_quality=0.0, price_sensitivity=0.25),\n",
    "        history_after=50,\n",
    "        agents=[\n",
    "            Qlearning(\n",
    "                discount=0.95, learning_rate=0.125, decision=DecreasingEpsilonGreedy(), marginal_cost=1.0, quality=2.0,\n",
    "            ),\n",
    "            Qlearning(\n",
    "                discount=0.95, learning_rate=0.125, decision=DecreasingEpsilonGreedy(), marginal_cost=1.0, quality=2.0,\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "    dqn_env.play_game()\n",
    "    Analyzer.analyze(dqn_env)\n",
    "\n",
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
